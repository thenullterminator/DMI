{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "DeepSVDD.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNrUrmz95Lbu",
        "outputId": "f94869de-e89d-4c5f-b98a-90785bd4dced"
      },
      "source": [
        "!pip install future\n",
        "!pip install goslate\n",
        "!pip install train\n",
        "!pip install barbar"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (0.16.0)\n",
            "Collecting goslate\n",
            "  Downloading https://files.pythonhosted.org/packages/39/0b/50af938a1c3d4f4c595b6a22d37af11ebe666246b05a1a97573e8c8944e5/goslate-1.5.1.tar.gz\n",
            "Collecting futures\n",
            "  Downloading https://files.pythonhosted.org/packages/05/80/f41cca0ea1ff69bce7e7a7d76182b47bb4e1a494380a532af3e8ee70b9ec/futures-3.1.1-py3-none-any.whl\n",
            "Building wheels for collected packages: goslate\n",
            "  Building wheel for goslate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for goslate: filename=goslate-1.5.1-cp37-none-any.whl size=11376 sha256=afe5eb4cc517cdba5ad250e51a3da6f66a425bd4b205de59fa8fb6b1fab4f324\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/7f/28/6f52271012a7649b54b1a7adaae329b4246bbbf9d1e4f6e51a\n",
            "Successfully built goslate\n",
            "Installing collected packages: futures, goslate\n",
            "Successfully installed futures-3.1.1 goslate-1.5.1\n",
            "Collecting train\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/bd/03ef37dfb2f0550f1fa43423bf8a2c833d1833ce1d90eb71dee05131eee2/train-0.0.5.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from train) (1.19.5)\n",
            "Building wheels for collected packages: train\n",
            "  Building wheel for train (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for train: filename=train-0.0.5-cp37-none-any.whl size=8505 sha256=b4c802de64c023def5e02c1d7b747503a835363632d2de0223a3e86140a1b834\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/73/59/3adbf17bfc99047fd2c397d3425d1a5778227f1d6d48c0f9f8\n",
            "Successfully built train\n",
            "Installing collected packages: train\n",
            "Successfully installed train-0.0.5\n",
            "Collecting barbar\n",
            "  Downloading https://files.pythonhosted.org/packages/48/1f/9b69ce144f484cfa00feb09fa752139658961de6303ea592487738d0b53c/barbar-0.2.1-py3-none-any.whl\n",
            "Installing collected packages: barbar\n",
            "Successfully installed barbar-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijA3reawu0tW",
        "outputId": "fad42554-65f3-4ea5-e2dd-9b7af220dd38"
      },
      "source": [
        "from train import TrainerDeepSVDD\n",
        "from preprocess import get_mnist\n",
        "from test import eval\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as myplot\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class argum:\n",
        "    pretrain = True\n",
        "    num_epochs = 150\n",
        "    num_epochs_ae = 150\n",
        "    batch_size = 200\n",
        "    normal_class = 1\n",
        "    latent_dim = 32\n",
        "    patience = 50\n",
        "    lr = 1e-4\n",
        "    lr_ae = 1e-4\n",
        "    lr_milestones = [50]\n",
        "    weight_decay = 0.5e-6\n",
        "    weight_decay_ae = 0.5e-3\n",
        "    \n",
        "weight_path = '/content/weights'\n",
        "if not os.path.isdir(weight_path): \n",
        "  os.mkdir(weight_path)\n",
        "\n",
        "para_path = '/content/weights/pretrained_parameters.pth'\n",
        "if not os.path.isfile(para_path):\n",
        "  with open('/content/weights/pretrained_parameters.pth', 'w') as fp:\n",
        "      pass\n",
        "\n",
        "dvc = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "A = argum()\n",
        "dt = get_mnist(A)\n",
        "dpSVDD = TrainerDeepSVDD(A, dt, dvc)\n",
        "\n",
        "if A.pretrain:\n",
        "    dpSVDD.pretrain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 164.241\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 120.484\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 86.048\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 62.057\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 46.508\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 36.256\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 29.208\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 24.156\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 20.402\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 17.541\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 15.324\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 13.568\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 12.156\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 10.999\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 10.027\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 9.205\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 8.498\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 7.885\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 7.347\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 6.873\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 20, Loss: 6.453\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 21, Loss: 6.077\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 22, Loss: 5.738\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 23, Loss: 5.432\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 24, Loss: 5.153\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 25, Loss: 4.893\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 26, Loss: 4.648\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 27, Loss: 4.423\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 28, Loss: 4.223\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 29, Loss: 4.038\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 30, Loss: 3.866\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 31, Loss: 3.708\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 32, Loss: 3.560\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 33, Loss: 3.425\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 34, Loss: 3.297\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 35, Loss: 3.174\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 36, Loss: 3.054\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 37, Loss: 2.941\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 38, Loss: 2.841\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 39, Loss: 2.748\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 40, Loss: 2.661\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 41, Loss: 2.579\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 42, Loss: 2.501\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 43, Loss: 2.429\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 44, Loss: 2.363\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 45, Loss: 2.301\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 46, Loss: 2.242\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 47, Loss: 2.186\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 48, Loss: 2.133\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 49, Loss: 2.083\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 50, Loss: 2.055\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 51, Loss: 2.049\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 52, Loss: 2.045\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 53, Loss: 2.041\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 54, Loss: 2.036\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 55, Loss: 2.030\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 56, Loss: 2.025\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 57, Loss: 2.022\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 58, Loss: 2.015\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 59, Loss: 2.010\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 60, Loss: 2.005\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 61, Loss: 1.999\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 62, Loss: 1.995\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 63, Loss: 1.988\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 64, Loss: 1.983\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 65, Loss: 1.978\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 66, Loss: 1.971\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 67, Loss: 1.967\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 68, Loss: 1.960\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 69, Loss: 1.954\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 70, Loss: 1.949\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 71, Loss: 1.943\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 72, Loss: 1.936\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 73, Loss: 1.930\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 74, Loss: 1.925\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 75, Loss: 1.919\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 76, Loss: 1.913\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 77, Loss: 1.907\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 78, Loss: 1.900\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 79, Loss: 1.895\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 80, Loss: 1.888\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 81, Loss: 1.882\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 82, Loss: 1.876\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 83, Loss: 1.870\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 84, Loss: 1.864\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 85, Loss: 1.855\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 86, Loss: 1.849\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 87, Loss: 1.843\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 88, Loss: 1.837\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 89, Loss: 1.830\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 90, Loss: 1.824\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 91, Loss: 1.816\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 92, Loss: 1.809\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 93, Loss: 1.803\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 94, Loss: 1.795\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 95, Loss: 1.790\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 96, Loss: 1.782\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 97, Loss: 1.776\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 98, Loss: 1.769\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 99, Loss: 1.761\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 100, Loss: 1.755\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 101, Loss: 1.748\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 102, Loss: 1.742\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 103, Loss: 1.734\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 104, Loss: 1.725\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 105, Loss: 1.719\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 106, Loss: 1.713\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 107, Loss: 1.705\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 108, Loss: 1.697\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 109, Loss: 1.691\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 110, Loss: 1.683\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 111, Loss: 1.676\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 112, Loss: 1.669\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 113, Loss: 1.662\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 114, Loss: 1.654\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 115, Loss: 1.647\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 116, Loss: 1.640\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 117, Loss: 1.633\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 118, Loss: 1.625\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 119, Loss: 1.617\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 120, Loss: 1.610\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 121, Loss: 1.604\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 122, Loss: 1.596\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 123, Loss: 1.588\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 124, Loss: 1.581\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 125, Loss: 1.574\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 126, Loss: 1.567\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 127, Loss: 1.559\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 128, Loss: 1.551\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 129, Loss: 1.545\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 130, Loss: 1.537\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 131, Loss: 1.529\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 132, Loss: 1.523\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 133, Loss: 1.514\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 134, Loss: 1.508\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 135, Loss: 1.501\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 136, Loss: 1.492\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 137, Loss: 1.485\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 138, Loss: 1.477\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 139, Loss: 1.471\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 140, Loss: 1.463\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 141, Loss: 1.455\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 142, Loss: 1.448\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 143, Loss: 1.440\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 144, Loss: 1.433\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 145, Loss: 1.426\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 146, Loss: 1.419\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 147, Loss: 1.411\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 148, Loss: 1.404\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 149, Loss: 1.397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K08l3tRn-1fE",
        "outputId": "07dce6d9-b266-49d7-8725-5af45ef3a46f"
      },
      "source": [
        "dpSVDD.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.314\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.089\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.039\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.024\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.018\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.015\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.012\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.011\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.009\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.008\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.008\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.007\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 20, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 21, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 22, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 23, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 24, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 25, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 26, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 27, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 28, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 29, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 30, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 31, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 32, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 33, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 34, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 35, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 36, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 37, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 38, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 39, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 40, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 41, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 42, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 43, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 44, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 45, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 46, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 47, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 48, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 49, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 50, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 51, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 52, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 53, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 54, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 55, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 56, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 57, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 58, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 59, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 60, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 61, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 62, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 63, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 64, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 65, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 66, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 67, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 68, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 69, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 70, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 71, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 72, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 73, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 74, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 75, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 76, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 77, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 78, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 79, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 80, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 81, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 82, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 83, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 84, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 85, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 86, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 87, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 88, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 89, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 90, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 91, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 92, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 93, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 94, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 95, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 96, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 97, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 98, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 99, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 100, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 101, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 102, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 103, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 104, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 105, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 106, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 107, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 108, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 109, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 110, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 111, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 112, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 113, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 114, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 115, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 116, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 117, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 118, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 119, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 120, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 121, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 122, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 123, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 124, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 125, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 126, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 127, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 128, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 129, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 130, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 131, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 132, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 133, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 134, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 135, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 136, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 137, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 138, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 139, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 140, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 141, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 142, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 143, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 144, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 145, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 146, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 147, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 148, Loss: 0.001\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 149, Loss: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KriHZ7eu0tY",
        "outputId": "cbb67685-e992-451d-c047-dd45188941ea"
      },
      "source": [
        "label, scores = eval(dpSVDD.net, dpSVDD.c, dt[1],dvc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "ROC AUC score: 99.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "6cELeVonu0tY",
        "outputId": "fbe90550-87c1-4f54-9e79-7e77b738de57"
      },
      "source": [
        "inline = scores[np.where(label==0)[0]]\n",
        "outline = scores[np.where(label==1)[0]]\n",
        "\n",
        "actual_inline = pd.DataFrame(inline, columns=['Inlier'])\n",
        "actual_outline = pd.DataFrame(outline, columns=['Outlier'])\n",
        "\n",
        "fig, ax = myplot.subplots()\n",
        "actual_inline.plot.kde(ax = ax, legend = True, title = 'Outliers vs Inliers (Deep SVDD)')\n",
        "actual_outline.plot.kde(ax = ax, legend = True)\n",
        "myplot.xlim(-0.05, 0.08)\n",
        "ax.grid(axis = 'X')\n",
        "ax.grid(axis = 'Y')\n",
        "myplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}